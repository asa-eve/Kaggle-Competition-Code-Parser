{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff8f510",
   "metadata": {},
   "source": [
    "## Functions from src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a470fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import nbformat\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "\n",
    "\n",
    "def PARSING_COMPETITION_NOTEBOOKS(COMPETITION_URL, SORT_BY='public score', PYTHON_ONLY=True, excludeNonAccessedDatasources=True, NOTEBOOKS_AMOUNT = 5, config = {}):\n",
    "    \n",
    "    columns = [\"notebook_name\", \"notebook_url\", \"public_score\", \"private_score\", \"medal\", \"upvotes\", \"views\", \"run_time_info\", \"last_updated\", \"notebook_full_text\", \"code_text\", \"markdowns_text\", \"input_datasources\", \"python_libraries\"]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('PARSING LINKS,  Time = ', 0)\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # BLOCKING ALL DOWNLOADS\n",
    "    firefox_options = webdriver.FirefoxOptions()\n",
    "    firefox_options.set_preference(\"browser.download.folderList\", 2)  # Use the last directory specified for downloads\n",
    "    firefox_options.set_preference(\"browser.download.dir\", \"./downloads\")  # Specify the download directory [directory does not exists -> not downloading at all]\n",
    "    firefox_options.set_preference(\"browser.download.useDownloadDir\", True)  # Use the specified download directory\n",
    "    firefox_options.set_preference(\"browser.download.manager.showWhenStarting\", False)  # Do not show download manager\n",
    "    firefox_options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/octet-stream\")  # Prevent automatic download of certain file types\n",
    "    firefox_options.headless = True  # Run in headless mode to hide browser window\n",
    "    driver = webdriver.Firefox(options=firefox_options)\n",
    "\n",
    "    # Load the webpage\n",
    "    driver.get(COMPETITION_URL)\n",
    "    \n",
    "    # # Wait for some time to allow dynamic content to load (you can adjust the time accordingly)\n",
    "    # time.sleep(2)\n",
    "    \n",
    "    # finding COMPETITION ID through the header (banner image)\n",
    "    WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, \"//img[contains(@src, '/competitions/') and contains(@src, '/images/header')]\")))\n",
    "    required_str = driver.find_element(By.XPATH, \"//img[contains(@src, '/competitions/') and contains(@src, '/images/header')]\").get_attribute('src')\n",
    "    competition_id = re.search(r'/(\\d+)/', required_str).group(1)\n",
    "    \n",
    "    \n",
    "    # FILTERS\n",
    "    COMPETITION_URL += f'/code?competitionId={competition_id}'\n",
    "    \n",
    "    if SORT_BY == 'public score':\n",
    "        COMPETITION_URL += '&sortBy=scoreDescending'\n",
    "    elif SORT_BY == 'vote count':\n",
    "        COMPETITION_URL += '&sortBy=voteCount'\n",
    "    else:\n",
    "        COMPETITION_URL += '&sortBy=commentCount'\n",
    "        \n",
    "    if PYTHON_ONLY: COMPETITION_URL += '&language=Python'\n",
    "    if excludeNonAccessedDatasources: COMPETITION_URL += '&excludeNonAccessedDatasources=true'\n",
    "    \n",
    "\n",
    "    # Load the webpage\n",
    "    driver.get(COMPETITION_URL)\n",
    "    # # Wait for some time to allow dynamic content to load (you can adjust the time accordingly)\n",
    "    # time.sleep(2)\n",
    "    \n",
    "    url_count = 0\n",
    "    MAX_NOTEBOOKS = NOTEBOOKS_AMOUNT\n",
    "\n",
    "    if config['MY NOTEBOOKS']:\n",
    "        notebook_links = config['CUSTOM NOTEBOOKS LIST']\n",
    "    else:\n",
    "        notebook_links = []\n",
    "            \n",
    "        WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"site-content\"]')))\n",
    "        element = driver.find_element(By.XPATH, '//*[@id=\"site-content\"]')\n",
    "        for i in range(MAX_NOTEBOOKS//15): \n",
    "            driver.execute_script(\"arguments[0].scrollBy(0, 50000);\", element)\n",
    "            time.sleep(1.5)\n",
    "        \n",
    "        # Find all list items within the object list\n",
    "        WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, '/code/') and contains(@href, '/comments')]\")))\n",
    "        list_items = driver.find_elements(By.XPATH, \"//a[contains(@href, '/code/') and contains(@href, '/comments')]\")\n",
    "        while (url_count < MAX_NOTEBOOKS):\n",
    "            notebook_links.append(list_items[url_count].get_attribute('href')[:-9])\n",
    "            url_count += 1\n",
    "            \n",
    "\n",
    "        \n",
    "    print('LINKS DONE,  Time = ', time.time() - start_time)\n",
    "        \n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    url_count = 0\n",
    "    for NOTEBOOK_URL in notebook_links:\n",
    "        url_count += 1\n",
    "        print(f'PARSING INFO FOR NOTEBOOK № {url_count}/{MAX_NOTEBOOKS},  Time = ', time.time() - start_time)\n",
    "        \n",
    "        # Load the webpage\n",
    "        driver.get(NOTEBOOK_URL)\n",
    "        # # Wait for some time to allow dynamic content to load (you can adjust the time accordingly)\n",
    "        # time.sleep(2)\n",
    "\n",
    "        # NOTEBOOK NAME\n",
    "        if config['NAME']:\n",
    "            try:\n",
    "                WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, \"//div[@wrap='hide']//h1\")))\n",
    "                notebook_name = driver.find_element(By.XPATH, \"//div[@wrap='hide']//h1\").text\n",
    "            except NoSuchElementException:\n",
    "                notebook_name = 'None'\n",
    "        else:\n",
    "            notebook_name = 'None'\n",
    "\n",
    "        # UPDATE LAST TIME\n",
    "        if config['UPDATE DATE']:\n",
    "            try:\n",
    "                update_date = driver.find_element(By.XPATH, \"//span[contains(@aria-label, 'ago')]\").get_attribute('aria-label')[:-4]\n",
    "                if update_date == 'a day': update_date = '1 day'\n",
    "                if update_date == 'a month': update_date = '1 month'\n",
    "                if update_date == 'a year': update_date = '1 year'\n",
    "            except NoSuchElementException:\n",
    "                update_date = 'None'\n",
    "        else:\n",
    "            update_date = 'None'\n",
    "\n",
    "        # UPVOTES\n",
    "        if config['UPVOTES']:\n",
    "            try:\n",
    "                upvotes_amount = int(driver.find_element(By.XPATH, \".//button[contains(@aria-label, 'votes')]\").text)\n",
    "            except NoSuchElementException:\n",
    "                upvotes_amount = 'None'\n",
    "        else:\n",
    "            upvotes_amount = 'None'\n",
    "\n",
    "        # MEDAL\n",
    "        if config['MEDAL']:\n",
    "            try:\n",
    "                medal = driver.find_element(By.XPATH, \"//img[contains(@src, '/static/images/medals/notebooks/') and contains(@src, '.png')]\").get_attribute('alt')[:-6]\n",
    "            except NoSuchElementException:\n",
    "                medal = 'None'\n",
    "        else:\n",
    "            medal = 'None'\n",
    "\n",
    "        # PUBLIC SCORE\n",
    "        if config['PUBLIC SCORE']:\n",
    "            try:\n",
    "                element = driver.find_element(By.XPATH, f\"//*[text()='Public Score']\") \n",
    "                parent = element.find_element(By.XPATH, \"..\")\n",
    "                public_score = float(parent.find_element(By.TAG_NAME, \"p\").text)\n",
    "            except NoSuchElementException:\n",
    "                public_score = 'None'\n",
    "        else:\n",
    "            public_score = 'None'\n",
    "\n",
    "        # PRIVATE SCORE\n",
    "        if config['PRIVATE SCORE']:\n",
    "            try:\n",
    "                element = driver.find_element(By.XPATH, f\"//*[text()='Private Score']\") \n",
    "                parent = element.find_element(By.XPATH, \"..\")\n",
    "                private_score = float(parent.find_element(By.TAG_NAME, \"p\").text)\n",
    "            except NoSuchElementException:\n",
    "                private_score = 'None'\n",
    "        else:\n",
    "            private_score = 'None'\n",
    "\n",
    "        # RUN TIME\n",
    "        if config['RUN TIME']:\n",
    "            try:\n",
    "                element = driver.find_element(By.XPATH, f\"//*[text()='Run']\") \n",
    "                parent = element.find_element(By.XPATH, \"..\")\n",
    "                run_time_info = parent.find_element(By.TAG_NAME, \"p\").text\n",
    "            except NoSuchElementException:\n",
    "                run_time_info = 'None'\n",
    "        else:\n",
    "            run_time_info = 'None'\n",
    "\n",
    "        # VIEWS\n",
    "        if config['VIEWS']:\n",
    "            try:\n",
    "                text = driver.find_element(By.XPATH, f\"//*[text()='views']\").text\n",
    "                views_amount = int(re.search(r'\\b(\\d+\\s*)+ VIEWS$', text).group(0).replace(' VIEWS', '').replace(' ', ''))\n",
    "            except NoSuchElementException:\n",
    "                views_amount = 'None'\n",
    "        else:\n",
    "            views_amount = 'None'\n",
    "    \n",
    "        # -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        time.sleep(1.5)\n",
    "        \n",
    "        # click ':'\n",
    "        inner_html_content = \"more_vert\"\n",
    "        WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, f\"//*[contains(., '{inner_html_content}')]\")))\n",
    "        download_button = driver.find_element(By.XPATH, f\"//*[contains(., '{inner_html_content}')]\")   # specifying by 'button name' -> since CSS selector changes every time\n",
    "        download_button.click()\n",
    "\n",
    "        # click 'download'\n",
    "        innter_html_content = \"Download code\"\n",
    "        WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, f\"//*[contains(., '{inner_html_content}')]\")))\n",
    "        download_button = driver.find_element(By.XPATH, f\"//*[contains(., '{inner_html_content}')]\")   # specifying by 'button name' -> since CSS selector changes every time\n",
    "        download_button.click()\n",
    "        \n",
    "        # return logs\n",
    "        logs = driver.execute_script(\"var performance = window.performance || window.mozPerformance || window.msPerformance || window.webkitPerformance || {}; var network = performance.getEntries() || {}; return network;\")\n",
    "\n",
    "        for log in logs:\n",
    "            if log['name'].startswith('https://www.kaggleusercontent.com/kf/'):\n",
    "                kernel_link = log['name']\n",
    "                break\n",
    "                \n",
    "        kernel_id = re.search(r'/(\\d+)/', kernel_link).group(1)\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        url = NOTEBOOK_URL + \"/input\"\n",
    "        input_datasources = []\n",
    "\n",
    "        # Load the webpage\n",
    "        driver.get(url)\n",
    "        # # Wait for some time to allow dynamic content to load (you can adjust the time accordingly)\n",
    "        # time.sleep(2)\n",
    "\n",
    "        if config['DATA SOURCES']:\n",
    "            try:\n",
    "                WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, \"//p[text()='Data Sources']\")))\n",
    "\n",
    "                # Find all list items within the object list [using 'child element' Data Sources -> to find parent element -> that containts list of objects that we need [ul class list, li class element]]\n",
    "                list_of_elements = driver.find_element(By.XPATH, f\"//p[text()='Data Sources']\")\n",
    "                parent_element = list_of_elements.find_element(By.XPATH, \"../..\")\n",
    "                ul_element = parent_element.find_element(By.TAG_NAME, \"ul\")\n",
    "                list_of_elements = ul_element.find_elements(By.TAG_NAME, \"li\") \n",
    "                for el in list_of_elements:\n",
    "                    input_datasources.append(el.text[12:])\n",
    "\n",
    "            except TimeoutException:\n",
    "                input_datasources.append('None')\n",
    "        else:\n",
    "            input_datasources.append('None')\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # URL of the Jupyter notebook\n",
    "        kernel_link = f'https://www.kaggle.com/kernels/scriptcontent/{kernel_id}/download'\n",
    "\n",
    "        # Download the notebook\n",
    "        response = requests.get(kernel_link)\n",
    "        notebook_content = response.content\n",
    "\n",
    "        # Parse the notebook content with nbformat\n",
    "        notebook = nbformat.reads(notebook_content, as_version=4)\n",
    "\n",
    "        # Extract text from the notebook cells\n",
    "        all_text = \"\"\n",
    "        code_text = \"\"\n",
    "        markdown_text = \"\"\n",
    "\n",
    "        if config['NOTEBOOK CELLS']:\n",
    "            for cell in notebook.cells:\n",
    "                if cell.cell_type == 'code':\n",
    "                    # Include code cell content\n",
    "                    all_text += cell.source + '\\n'\n",
    "                    code_text += cell.source + '\\n'\n",
    "                elif cell.cell_type == 'markdown':\n",
    "                    # Include markdown cell content\n",
    "                    all_text += cell.source + '\\n'\n",
    "                    markdown_text += cell.source + '\\n'\n",
    "        \n",
    "        # Forming list of Python libraries\n",
    "        python_libraries = []\n",
    "        if config['PYTHON LIBRARIES']:\n",
    "            lines_of_code = list(filter(bool, code_text.split('\\n')))\n",
    "            for line in lines_of_code:\n",
    "                if line.startswith('import') or line.startswith('from'):\n",
    "                    python_libraries.append(line.split(' ')[1])\n",
    "            python_libraries = list(OrderedDict.fromkeys(python_libraries))\n",
    "        \n",
    "        # -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        df.loc[len(df.index)] = [notebook_name, NOTEBOOK_URL, public_score, private_score, medal, upvotes_amount, views_amount, run_time_info, update_date, all_text, code_text, markdown_text, input_datasources, python_libraries]\n",
    "        \n",
    "    # -------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    driver.quit()\n",
    "    end_time = time.time()\n",
    "    print(\"Elapsed time:\", end_time - start_time, \"seconds\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d962da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pkgutil\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "def get_valid_module_names():\n",
    "    # Get a set of valid Python module names\n",
    "    valid_module_names = set()\n",
    "    for _, module_name, _ in pkgutil.iter_modules():\n",
    "        valid_module_names.add(module_name)\n",
    "    return valid_module_names\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "def clean_library_names(library_names):\n",
    "    cleaned_names = []\n",
    "\n",
    "    # Get valid Python module names\n",
    "    valid_module_names = get_valid_module_names()\n",
    "\n",
    "    # Compile a regex pattern to match standalone words\n",
    "    pattern = re.compile(r'^[a-zA-Z_][a-zA-Z0-9_]*$')\n",
    "\n",
    "    for name in library_names:\n",
    "        # Check if the name is a valid Python module name\n",
    "        if name in valid_module_names:\n",
    "            cleaned_names.append(name)\n",
    "        else:\n",
    "            # Split the string by \".\", \":\", or \";\"\n",
    "            parts = re.split(r'[.:;]', name)\n",
    "            cleaned_names.append(parts[0])\n",
    "            \n",
    "#             cleaned_parts = []\n",
    "\n",
    "#             for part in parts:\n",
    "#                 # Check if the part is a standalone word\n",
    "#                 match = pattern.match(part)\n",
    "#                 if match:\n",
    "#                     cleaned_parts.append(part)\n",
    "\n",
    "#             # If there's only one part and it's a valid module name, append it\n",
    "#             if len(cleaned_parts) == 1 and cleaned_parts[0] in valid_module_names:\n",
    "#                 cleaned_names.append(cleaned_parts[0])\n",
    "\n",
    "    return cleaned_names\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "def clean_sublibrary_names(library_names):\n",
    "    cleaned_names = []\n",
    "\n",
    "    # Get valid Python module names\n",
    "    valid_module_names = get_valid_module_names()\n",
    "\n",
    "    # Compile a regex pattern to match standalone words\n",
    "    pattern = re.compile(r'^[a-zA-Z_][a-zA-Z0-9_]*$')\n",
    "\n",
    "    for name in library_names:\n",
    "        # Split the string by \".\", \":\", or \";\"\n",
    "        parts = re.split(r'[.:;]', name)\n",
    "        cleaned_parts = []\n",
    "\n",
    "        for part in parts:\n",
    "            # Check if the part is a valid Python module name\n",
    "            if part in valid_module_names:\n",
    "                cleaned_parts.append(part)\n",
    "            else:\n",
    "                # Check if the part is a standalone word\n",
    "                match = pattern.match(part)\n",
    "                if match:\n",
    "                    cleaned_parts.append(part)\n",
    "\n",
    "        # Join the cleaned parts back into a string and append it to the cleaned_names list\n",
    "        cleaned_name = \".\".join(cleaned_parts)\n",
    "        if cleaned_name:\n",
    "            cleaned_names.append(cleaned_name)\n",
    "\n",
    "    return cleaned_names\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "def count_strings(strings, unique_strings, NO_SUBLIBRARIES):\n",
    "    if NO_SUBLIBRARIES:\n",
    "        for idx in range(len(strings)):\n",
    "            parts = re.split(r'[.:;]', strings[idx])\n",
    "            strings[idx] = parts[0]\n",
    "    \n",
    "    string_dict = {}\n",
    "\n",
    "    for unique_string in unique_strings:\n",
    "        string_dict[unique_string] = strings.count(unique_string)\n",
    "\n",
    "    return string_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1cc8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Code_parsing(competition_url, SORT_BY, NOTEBOOKS_AMOUNT, CSVs_SAVING_DIR, NO_SUBLIBRARIES, config):\n",
    "    \n",
    "    for i in range(8): print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"============== KAGGLE CODE PARSING (Firefox browser ONLY) =================\")\n",
    "    print(\"SETTINGS\")\n",
    "    print(f\"        Competition url        -   {competition_url}\")\n",
    "    print(f\"        Sorting by             -   {SORT_BY}\")\n",
    "    print(f\"        Amount of notebooks    -   {NOTEBOOKS_AMOUNT}\")\n",
    "    print(f\"        Save to directory      -   {CSVs_SAVING_DIR}\")\n",
    "    print(f\"        Remove sublibraries    -   {'Yes'*bool(NO_SUBLIBRARIES) + 'No'*(1 - bool(NO_SUBLIBRARIES))}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    file_name = list(filter(bool, competition_url.split('/')))[-1]\n",
    "    full_file_name = f\"{file_name}_{SORT_BY.replace(' ', '')}_{NOTEBOOKS_AMOUNT}.csv\"\n",
    "\n",
    "    if os.path.exists(CSVs_SAVING_DIR + full_file_name):\n",
    "        print(\"Loading existing file\")\n",
    "        df = pd.read_csv(CSVs_SAVING_DIR + full_file_name)\n",
    "\n",
    "        # transform LISTS columns into LISTS (since saving transforms them into STRINGS)\n",
    "        df['python_libraries'] = df['python_libraries'].apply(ast.literal_eval)\n",
    "        df['input_datasources'] = df['input_datasources'].apply(ast.literal_eval)\n",
    "    else:\n",
    "        print(\"Parsing data from competition + Saving\")\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "        df = PARSING_COMPETITION_NOTEBOOKS(competition_url, SORT_BY, True, True, NOTEBOOKS_AMOUNT, config)  # COMPETITION_URL / SORT_BY / PYTHON_ONLY / excludeNonAccessedDatasources / NOTEBOOKS_AMOUNT\n",
    "\n",
    "        df.to_csv(CSVs_SAVING_DIR + full_file_name, index=False)\n",
    "        \n",
    "        \n",
    "    if bool(int(NO_SUBLIBRARIES)):\n",
    "        # changed a little big --- now 'gemma.config' considered as 'gemma' library (previously it wasn't considered at all)\n",
    "\n",
    "        all_python_libraries = list(OrderedDict.fromkeys(sum(df['python_libraries'], [])))\n",
    "        cleaned_all_python_libraries_repeated = clean_library_names(all_python_libraries)\n",
    "        cleaned_all_python_libraries= list(OrderedDict.fromkeys(cleaned_all_python_libraries_repeated, []))\n",
    "        print(\"        TOTAL AMOUNT OF LIBRARIES = \", len(cleaned_all_python_libraries))\n",
    "\n",
    "    else:\n",
    "        all_python_libraries = list(OrderedDict.fromkeys(sum(df['python_libraries'], [])))\n",
    "        cleaned_all_python_libraries = clean_sublibrary_names(all_python_libraries)\n",
    "        print(\"        TOTAL AMOUNT OF LIBRARIES + SUB-LIBRARIES = \", len(cleaned_all_python_libraries))\n",
    "        \n",
    "    print(\"\")\n",
    "    print(\"DONE\")\n",
    "    print(\"===========================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f647643",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "**What to set-up**:\n",
    "- `url` - competition url\n",
    "- `my_list` - can be filled with custom notebook URLS \n",
    "    - if filled, no need to change `sort_by` and `NOTEBOOK TO PARSE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bcaed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom parsing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============== KAGGLE CODE PARSING (Firefox browser ONLY) =================\n",
      "SETTINGS\n",
      "        Competition url        -   https://www.kaggle.com/competitions/nyc-taxi-trip-duration\n",
      "        Sorting by             -   vote count\n",
      "        Amount of notebooks    -   4\n",
      "        Save to directory      -   _DOWNLOADED NOTEBOOKS/\n",
      "        Remove sublibraries    -   No\n",
      "\n",
      "Parsing data from competition + Saving\n",
      "-----------------------------\n",
      "PARSING LINKS,  Time =  0\n",
      "LINKS DONE,  Time =  15.765558958053589\n",
      "PARSING INFO FOR NOTEBOOK № 1/4,  Time =  15.765558958053589\n",
      "PARSING INFO FOR NOTEBOOK № 2/4,  Time =  19.79638123512268\n",
      "PARSING INFO FOR NOTEBOOK № 3/4,  Time =  23.839372634887695\n",
      "PARSING INFO FOR NOTEBOOK № 4/4,  Time =  27.90632724761963\n",
      "Elapsed time: 34.18342685699463 seconds\n",
      "        TOTAL AMOUNT OF LIBRARIES + SUB-LIBRARIES =  0\n",
      "\n",
      "DONE\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "# SORT_BY = 'vote count' # 'public score', 'vote count', 'comment count'\n",
    "# NOTEBOOKS_AMOUNT = 10\n",
    "# competition_url = \"https://www.kaggle.com/competitions/ai-mathematical-olympiad-prize\" #\"https://www.kaggle.com/competitions/llm-prompt-recovery\"\n",
    "# CSVs_SAVING_DIR = \"Kaggle notebooks CSVs/\"\n",
    "# NO_SUBLIBRARIES = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# provide links to notebooks here\n",
    "my_list = [\n",
    "    #'https://www.kaggle.com/code/merckel/autoencoder-and-deep-features',\n",
    "    'https://www.kaggle.com/code/donniedarko/darktaxi-tripdurationprediction-lb-0-385',\n",
    "    'https://www.kaggle.com/code/jeffreycbw/nyc-taxi-trip-public-0-37399-private-0-37206',\n",
    "    'https://www.kaggle.com/code/quentinmonmousseau/ml-workflow-lightgbm-0-37-randomforest-0-39',\n",
    "    'https://www.kaggle.com/code/donniedarko/darktaxi-tripdurationprediction-lb-0-385',\n",
    "]    \n",
    "\n",
    "config = {\n",
    "    'NAME': True,\n",
    "    'UPDATE DATE': True,\n",
    "    'UPVOTES': True,\n",
    "    'MEDAL': False,\n",
    "    'PUBLIC SCORE': False,\n",
    "    'PRIVATE SCORE': False,\n",
    "    'RUN TIME': False,\n",
    "    'VIEWS': False, \n",
    "\n",
    "    \"NOTEBOOKS TO PARSE\": 5, \n",
    "\n",
    "    'DATA SOURCES': False,          # if any external data needed (not important)\n",
    "    'NOTEBOOK CELLS': True,         # should always be True\n",
    "    'PYTHON LIBRARIES': False,      # parse libraries names (required for visuals)\n",
    "}\n",
    "\n",
    "url  = 'https://www.kaggle.com/competitions/nyc-taxi-trip-duration'\n",
    "sort_by = 'vote count'\n",
    "\n",
    "saving_dir = '_DOWNLOADED NOTEBOOKS/'\n",
    "os.chdir('C:/_Github repositories')\n",
    "no_sublibraries = 0\n",
    "\n",
    "if len(my_list) != 0: \n",
    "    print(\"Custom parsing\")\n",
    "\n",
    "    config['MY NOTEBOOKS'] = True\n",
    "    config['CUSTOM NOTEBOOKS LIST'] = my_list\n",
    "\n",
    "    Code_parsing(url, sort_by, len(my_list), saving_dir, no_sublibraries, config)\n",
    "else:\n",
    "    print(f\"Top {config['NOTEBOOKS TO PARSE']} by {sort_by} parsing\")\n",
    "\n",
    "    config['MY NOTEBOOKS'] = True\n",
    "    config['CUSTOM NOTEBOOKS LIST'] = my_list\n",
    "\n",
    "    Code_parsing(url, sort_by, config['NOTEBOOKS TO PARSE'], saving_dir, no_sublibraries, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e8dde",
   "metadata": {},
   "source": [
    "## To txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb37708e",
   "metadata": {},
   "source": [
    "### EACH IN SEPARATE .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f3ff7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Base directory\n",
    "CSVs_SAVING_DIR = 'C:/_Github repositories/_DOWNLOADED NOTEBOOKS/'\n",
    "\n",
    "file_name = list(filter(bool, url.split('/')))[-1]\n",
    "\n",
    "# Read CSV\n",
    "if len(my_list) != 0:\n",
    "    csv_path = os.path.join(CSVs_SAVING_DIR, f\"{file_name}_{sort_by.replace(' ', '')}_{len(my_list)}.csv\")\n",
    "else:   \n",
    "    csv_path = os.path.join(CSVs_SAVING_DIR, f\"{file_name}_{sort_by.replace(' ', '')}_{config['NOTEBOOKS TO PARSE']}.csv\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df['python_libraries'] = df['python_libraries'].apply(ast.literal_eval)\n",
    "df['input_datasources'] = df['input_datasources'].apply(ast.literal_eval)\n",
    "\n",
    "# Create subfolder for this notebook batch\n",
    "main_folder = os.path.join(CSVs_SAVING_DIR, file_name)\n",
    "os.makedirs(main_folder, exist_ok=True)\n",
    "\n",
    "# Create sub-subfolder for individual notebook .txt files\n",
    "individual_folder = os.path.join(main_folder, 'individual_notebooks')\n",
    "os.makedirs(individual_folder, exist_ok=True)\n",
    "\n",
    "# Save each notebook's full text as a separate .txt file\n",
    "for i, row in df.iterrows():\n",
    "    filename = f\"{row['notebook_name']}.txt\"\n",
    "    safe_filename = \"\".join(c for c in filename if c.isalnum() or c in (' ', '.', '_')).rstrip()\n",
    "    \n",
    "    txt_path = os.path.join(individual_folder, safe_filename)\n",
    "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(row['notebook_full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e7a4b",
   "metadata": {},
   "source": [
    "### ALL IN 1 .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3da9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Define base directory\n",
    "CSVs_SAVING_DIR = 'C:/_Github repositories/_DOWNLOADED NOTEBOOKS/'\n",
    "file_name = list(filter(bool, url.split('/')))[-1]\n",
    "\n",
    "# Create subfolder for .txt files\n",
    "subfolder_path = os.path.join(CSVs_SAVING_DIR, file_name)\n",
    "os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "# Read the CSV file\n",
    "# Read CSV\n",
    "if len(my_list) != 0:\n",
    "    csv_path = os.path.join(CSVs_SAVING_DIR, f\"{file_name}_{sort_by.replace(' ', '')}_{len(my_list)}.csv\")\n",
    "else:   \n",
    "    csv_path = os.path.join(CSVs_SAVING_DIR, f\"{file_name}_{sort_by.replace(' ', '')}_{config['NOTEBOOKS TO PARSE']}.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert stringified lists back to actual lists\n",
    "df['python_libraries'] = df['python_libraries'].apply(ast.literal_eval)\n",
    "df['input_datasources'] = df['input_datasources'].apply(ast.literal_eval)\n",
    "\n",
    "# Save all 'notebook_full_text' entries into one combined .txt file inside the subfolder\n",
    "combined_txt_path = os.path.join(subfolder_path, 'combined_notebooks.txt')\n",
    "with open(combined_txt_path, 'w', encoding='utf-8') as f:\n",
    "    for i, row in df.iterrows():\n",
    "        f.write(f\"Notebook: {row['notebook_name']}\\n\")\n",
    "        f.write(row['notebook_full_text'])\n",
    "        f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")  # Separator between notebooks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_pactice_3-10-18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
